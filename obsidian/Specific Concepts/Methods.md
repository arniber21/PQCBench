# Methods
We shall begin by stating the choices in the design of our experiment, then following up this overview with a justification of these choices. 

In order to gauge the feasibility of the NIST-selected post-quantum algorithms, we opt to  test two algorithms: Kyber and ECC-KEM (the Elliptic Curve Integrated Encryption Scheme, or ECIES). Our approach is purely quantitative in nature, collecting the following performance metrics: CPU usage, as a percentage; memory usage, in megabytes; and time, in seconds. We use a Haswell-based 12-core Intel Xeon processor; however, we opt to test each algorithm on a single thread.  Additionally, in order to guarantee a level of separation from the base operating system, we run each benchmark inside a Docker virtual machine.
## Why Quantitative
In line with most cryptographic benchmarks, we choose a quantitative study primarily to optimize ease of use. Quantitative figures are easily available when measuring CPU and memory usage; therefore, in order to take advantage of our mathematical toolkit to analyze figures, we opt to use quantitative data. Crucially, we plan to collect data at a scale in which manual data analysis is not feasible - quantitative data allows us to computationally analyze these results simultaneously to find the patterns we seek efficiently. 
## Past Methods
We primarily attempt to adhere to PQ-Crystals' own methodology for benchmarking Kyber in the design of this experiment. However, we must reconcile this goal with the reality that Kyber and ECIES are very different algorithms; therefore, because we must ensure that there are minimal differences in the benchmarking setup between the two algorithms, we opt to build our own benchmarking script for both ECIES and KEM. Let us describe the individual benchmarking script for each algorithm a *test saddle*, and relegate the benchmark label to the script producing the performance data. 

Our experiment, then, in line with other experiments, designs a test saddle and runs this test saddle with a benchmark for each algorithm. We then compare the results for each of the discrete combinations we wish to test. One may notice a large discrepancy in our experiment compared to other experiments: whereas the majority of other experiments measure CPU performance primarily through cycles, we opt to measure CPU performance as a percentage of total CPU usage. We recognize that there are several advantages to using cycles over a percentage of total usage. Cycles are far more standardized and accurate when compared percentages. However, recall that the focus of this experiment is not to compute a direct performance comparison between the two algorithms; rather, we wish to gauge how *viable* post-quantum algorithms are. Also recall that it is well established that compute limitations on post-quantum algorithms are not limited even by small CPUs; rather, we are far more concerned with memory usage. As a consequence of testing two very different algorithms on Linux systems, it therefore makes sense to prioritize the convenience of gathering memory usage. In essence, we are sacrificing good CPU usage data in order to gain more reliable and useful information in the form of memory data. 

## Why Kyber
It is true that NIST has selected other post-quantum algorithms: namely, Dilithium; however, Dilithium and Kyber share the same mathematical foundation (the Learning With Errors problem). If the purpose of this study were to be a direct performance comparison between different cryptographic algorithms, we would be obligated to test both Dilithium and Kyber. However, because we solely wish to test the feasibility of these algorithms rather than their direct performance, we conclude that it is adequate to simply test Kyber. Additionally, PQ-Crystals, the organization who developed both Kyber and Dilithium, provides direct CPU performance comparisons under small payloads; therefore, a performance comparison of the two is not necessary. 
## Why Single-Threaded
Also note that this study does not attempt to use multi-threading to optimize either cipher. We choose not to take advantage of multithreading primarily to objectify the study as much as possible. This study does not make any assertions on the effect of effects of multithreading on the effectiveness of either ECIES or KEM, nor does it wish to. Because multithreaded approaches to neither are standardized, we wish to relegate the potential effects of multithreading to future studies and instead solely focus on the feasibility of the official specifications of Kyber and ECIES. Attempting to multithread each would introduce significant differences between the two, compromising any attempt at a comparison. We elaborate on this choice and its consequences in the limitations section of this paper. 
## Why Docker
Earlier, we note that we use Docker as a virtualization platform. We will briefly justify the use of a virtualization platform. A platform such as Docker allows us to minimize performance overhead while retaining a key advantage of virtualization: independence from the host system. This allows each test to remain as independent and equal as possible, with little to no difference in the starting environments. This can remove sources of random bias, such as the operating system mysteriously choosing to allocate more resources to one process over the other. The primary disadvantage of virtualization is the significant performance overhead induced by a virtual platform; however, Docker is minimal as to minimize this effect to where the loss in performance is negligible. Therefore, we opt to use Docker to increase the consistency of our study. 